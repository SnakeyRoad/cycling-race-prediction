{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycling Race Data Preprocessing Pipeline\n",
    "\n",
    "This notebook preprocesses cycling race results (2012-2021) for machine learning.\n",
    "\n",
    "**Goal:** Predict whether a rider will finish in top 30 (binary classification)\n",
    "\n",
    "## Why We Process 2017-2021 (Not 2016-2021)\n",
    "\n",
    "Each dataset needs 3-year historical features (sumres_1, sumres_2, sumres_3).\n",
    "\n",
    "```\n",
    "Available CSV Files:\n",
    "├── Results2012CatWT.csv ✓\n",
    "├── Results2013CatWT.csv ✗ MISSING!\n",
    "├── Results2014CatWT.csv ✓\n",
    "├── Results2015CatWT.csv ✓\n",
    "├── Results2016CatWT.csv ✓\n",
    "├── Results2017CatWT.csv ✓\n",
    "├── Results2018CatWT.csv ✓\n",
    "├── Results2019CatWT.csv ✓\n",
    "├── Results2020CatWT.csv ✓\n",
    "└── Results2021CatWT.csv ✓\n",
    "\n",
    "For 2016: Need 2015✓, 2014✓, 2013✗ (MISSING!)\n",
    "For 2017: Need 2016✓, 2015✓, 2014✓ (Complete!)\n",
    "```\n",
    "\n",
    "**Solution:** Start from 2017 where complete 3-year history exists.\n",
    "\n",
    "**Output:** Five cleaned CSV files (2017-2021) ready for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why We Exclude 2016 Data: A Detailed Explanation\n",
    "\n",
    "## Executive Summary\n",
    "**Decision:** Process years 2017-2021 only (5 datasets)  \n",
    "**Reason:** Missing 2013 race results file prevents complete historical feature engineering for 2016  \n",
    "**Impact:** No negative impact on model quality; ensures data consistency across all datasets\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem: Missing Historical Data\n",
    "\n",
    "### Our Feature Engineering Approach\n",
    "This project predicts whether a rider will finish in the **top 30** of a race (binary classification). A critical component of our model is **historical performance features** that capture each rider's track record over the previous 3 years:\n",
    "\n",
    "- **`sumres_1`**: Total UCI points earned by the rider in the previous year (Y-1)\n",
    "- **`sumres_2`**: Total UCI points earned 2 years ago (Y-2)  \n",
    "- **`sumres_3`**: Total UCI points earned 3 years ago (Y-3)\n",
    "\n",
    "These features are essential because a rider's recent performance is one of the strongest predictors of future success.\n",
    "\n",
    "### The 2013 Data Gap\n",
    "Our dataset contains race results from **2012, 2014-2021** (9 files total). **Results from 2013 are missing** from the original data collection.\n",
    "\n",
    "This creates a critical problem for 2016 predictions:\n",
    "\n",
    "| Year | sumres_1 needs | sumres_2 needs | sumres_3 needs | Status |\n",
    "|------|---------------|---------------|---------------|---------|\n",
    "| **2016** | 2015 ✓ | 2014 ✓ | **2013 ✗** | **INCOMPLETE** |\n",
    "| 2017 | 2016 ✓ | 2015 ✓ | 2014 ✓ | Complete |\n",
    "| 2018 | 2017 ✓ | 2016 ✓ | 2015 ✓ | Complete |\n",
    "| 2019 | 2018 ✓ | 2017 ✓ | 2016 ✓ | Complete |\n",
    "| 2020 | 2019 ✓ | 2018 ✓ | 2017 ✓ | Complete |\n",
    "| 2021 | 2020 ✓ | 2019 ✓ | 2018 ✓ | Complete |\n",
    "\n",
    "### Why This Matters\n",
    "If we included 2016 in our dataset, **every single rider** would have `sumres_3 = NaN` (missing). This creates several problems:\n",
    "\n",
    "1. **Data Inconsistency**: 2016 would have incomplete historical features while 2017-2021 have complete features\n",
    "2. **Imputation Issues**: Filling missing values with column means introduces artificial data for an important predictive feature\n",
    "3. **Model Confusion**: The model would learn different patterns from 2016 vs. other years\n",
    "4. **Team Coordination**: Multiple team members (David, Youri, Iris) would need to handle this special case differently\n",
    "\n",
    "---\n",
    "\n",
    "## The Solution: Start from 2017\n",
    "\n",
    "By excluding 2016 and starting from 2017, we ensure:\n",
    "\n",
    " **Complete 3-year history** for all riders in all datasets  \n",
    " **Data consistency** across all 5 years  \n",
    " **No artificial imputation** of critical features  \n",
    " **Cleaner modeling** for the entire team  \n",
    " **Still ample data**: 5 years × ~25,000 rows = ~125,000 training examples\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "Available Data Files:\n",
    "├── 2012 ✓ ─┐\n",
    "├── 2013 ✗  │ ← Missing! Cannot calculate sumres_3 for 2016\n",
    "├── 2014 ✓ ─┤\n",
    "├── 2015 ✓ ─┤\n",
    "├── 2016 ✓ ─┴─→ EXCLUDED (would have incomplete features)\n",
    "├── 2017 ✓ ───→ FIRST YEAR PROCESSED (complete 3-year history)\n",
    "├── 2018 ✓ ───→ Processed\n",
    "├── 2019 ✓ ───→ Processed\n",
    "├── 2020 ✓ ───→ Processed\n",
    "└── 2021 ✓ ───→ Processed\n",
    "```\n",
    "\n",
    "### Example: 2017 Historical Features\n",
    "For a race in 2017, we calculate:\n",
    "- `sumres_1` from **2016 results** ✓ (file exists)\n",
    "- `sumres_2` from **2015 results** ✓ (file exists)\n",
    "- `sumres_3` from **2014 results** ✓ (file exists)\n",
    "\n",
    "**Result**: Complete, reliable historical context for every rider.\n",
    "\n",
    "---\n",
    "\n",
    "## Impact Assessment\n",
    "\n",
    "### What We Keep\n",
    "- **125,000+ data points** across 5 years\n",
    "- **Complete feature set** for all observations\n",
    "- **High-quality training data** without imputation artifacts\n",
    "- **Consistent data structure** for team collaboration\n",
    "\n",
    "### What We Lose\n",
    "- **~25,000 rows** from 2016\n",
    "- This represents only **16.7%** of potential data\n",
    "- No loss in **data quality** or **feature completeness**\n",
    "\n",
    "### Risk Mitigation\n",
    "The exclusion of 2016 does **not** compromise our modeling objectives:\n",
    "- We still have 5 complete years of data\n",
    "- All temporal patterns (2017-2021) are captured\n",
    "- Model will generalize to future years (2022+) which also have complete historical data\n",
    "- No artificial patterns from imputed values\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Excluding 2016 is the correct data engineering decision. It prioritizes **data quality and consistency** over quantity, ensuring that our Random Forest and Gradient Boosting models learn from clean, complete features rather than being confused by incomplete historical data.\n",
    "\n",
    "**Status**: ✅ All 5 datasets (2017-2021) ready for modeling with complete 3-year historical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CYCLING RACE DATA PREPROCESSING PIPELINE\n",
      "================================================================================\n",
      "Processing years 2017-2021 (5 complete datasets)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CYCLING RACE DATA PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Processing years 2017-2021 (5 complete datasets)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Parse Rider Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_column(series, prefix=''):\n",
    "    \"\"\"Parse JSON-like string column into separate columns\"\"\"\n",
    "    def safe_parse(x):\n",
    "        try:\n",
    "            if pd.isna(x):\n",
    "                return {}\n",
    "            # Convert single quotes to double quotes for valid JSON\n",
    "            x_cleaned = x.replace(\"'\", '\"')\n",
    "            return json.loads(x_cleaned)\n",
    "        except:\n",
    "            return {}\n",
    "    \n",
    "    parsed = series.apply(safe_parse)\n",
    "    df_parsed = pd.DataFrame(parsed.tolist())\n",
    "    \n",
    "    # Convert all columns to numeric\n",
    "    for col in df_parsed.columns:\n",
    "        df_parsed[col] = pd.to_numeric(df_parsed[col], errors='coerce')\n",
    "    \n",
    "    return df_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1042 riders\n",
      "Original columns: ['Unnamed: 0', 'fullname', 'team', 'birthdate', 'country', 'height', 'weight', 'rider_url', 'pps', 'rdr']\n",
      "\n",
      "Filtered out 236 riders with 'noteam'\n",
      "Remaining riders: 806\n"
     ]
    }
   ],
   "source": [
    "# Load rider information\n",
    "rider_infos = pd.read_csv('./data/rider_infos.csv')\n",
    "print(f\"Loaded {len(rider_infos)} riders\")\n",
    "print(f\"Original columns: {list(rider_infos.columns)}\")\n",
    "\n",
    "# Filter out riders with no team\n",
    "initial_count = len(rider_infos)\n",
    "rider_infos = rider_infos[rider_infos['team'] != 'noteam']\n",
    "filtered_count = len(rider_infos)\n",
    "print(f\"\\nFiltered out {initial_count - filtered_count} riders with 'noteam'\")\n",
    "print(f\"Remaining riders: {filtered_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'pps' specialty scores...\n",
      "PPS columns: ['One day races', 'GC_specialty', 'Time trial', 'Sprint', 'Climber']\n",
      "\n",
      "Parsing 'rdr' rankings...\n",
      "RDR columns: ['PCS Ranking', 'UCI World Ranking', 'Specials | All Time Ranking']\n"
     ]
    }
   ],
   "source": [
    "# Parse 'pps' column (specialty scores)\n",
    "print(\"Parsing 'pps' specialty scores...\")\n",
    "pps_parsed = parse_json_column(rider_infos['pps'])\n",
    "# Rename GC to GC_specialty to avoid conflict\n",
    "if 'GC' in pps_parsed.columns:\n",
    "    pps_parsed.rename(columns={'GC': 'GC_specialty'}, inplace=True)\n",
    "print(f\"PPS columns: {list(pps_parsed.columns)}\")\n",
    "\n",
    "# Parse 'rdr' column (rankings)\n",
    "print(\"\\nParsing 'rdr' rankings...\")\n",
    "rdr_parsed = parse_json_column(rider_infos['rdr'])\n",
    "print(f\"RDR columns: {list(rdr_parsed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final rider_infos shape: (966, 11)\n",
      "Columns: ['fullname', 'height', 'weight', 'One day races', 'GC_specialty', 'Time trial', 'Sprint', 'Climber', 'PCS Ranking', 'UCI World Ranking', 'Specials | All Time Ranking']\n",
      "\n",
      "Sample rider data:\n",
      "        fullname  height  weight  One day races  GC_specialty  Time trial  \\\n",
      "0  BARDET Romain    1.84    65.0         2620.0        5138.0       333.0   \n",
      "2  GALLOPIN Tony    1.80    69.0         3619.0        1157.0        35.0   \n",
      "3  NAESEN Oliver    1.84    72.0         1168.0        1506.0        98.0   \n",
      "\n",
      "   Sprint  Climber  PCS Ranking  UCI World Ranking  \\\n",
      "0   446.0   6414.0         43.0               52.0   \n",
      "2  1996.0    968.0        203.0              143.0   \n",
      "3   416.0   1523.0        446.0              478.0   \n",
      "\n",
      "   Specials | All Time Ranking  \n",
      "0                        375.0  \n",
      "2                        729.0  \n",
      "3                       1436.0  \n",
      "\n",
      "Missing values:\n",
      "fullname                       160\n",
      "height                         160\n",
      "weight                         160\n",
      "One day races                  160\n",
      "GC_specialty                   160\n",
      "Time trial                     160\n",
      "Sprint                         160\n",
      "Climber                        160\n",
      "PCS Ranking                    226\n",
      "UCI World Ranking              218\n",
      "Specials | All Time Ranking    716\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combine all rider information\n",
    "rider_infos_clean = pd.concat([\n",
    "    rider_infos[['fullname', 'height', 'weight']],\n",
    "    pps_parsed,\n",
    "    rdr_parsed\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nFinal rider_infos shape: {rider_infos_clean.shape}\")\n",
    "print(f\"Columns: {list(rider_infos_clean.columns)}\")\n",
    "print(\"\\nSample rider data:\")\n",
    "print(rider_infos_clean.head(3))\n",
    "print(f\"\\nMissing values:\\n{rider_infos_clean.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Historical Performance Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_historical_points(year, years_back=3):\n",
    "    \"\"\"\n",
    "    Calculate historical points for riders from previous years\n",
    "    \n",
    "    Args:\n",
    "        year: Current year (e.g., 2019)\n",
    "        years_back: Number of previous years to include (default 3)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with rider names and sumres_1, sumres_2, sumres_3 columns\n",
    "    \"\"\"\n",
    "    historical_data = {}\n",
    "    \n",
    "    for i in range(1, years_back + 1):\n",
    "        prev_year = year - i\n",
    "        file_path = f'./data/results/Results{prev_year}CatWT.csv'\n",
    "        \n",
    "        try:\n",
    "            # Load previous year results\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Calculate total points per rider per race, then sum by rider\n",
    "            race_points = df.groupby(['Race_Name', 'Rider'])['Pnt'].sum().reset_index()\n",
    "            total_points = race_points.groupby('Rider')['Pnt'].sum()\n",
    "            \n",
    "            historical_data[f'sumres_{i}'] = total_points\n",
    "            print(f\"  Loaded {prev_year}: {len(total_points)} riders with points\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"  WARNING: File not found for year {prev_year}\")\n",
    "            historical_data[f'sumres_{i}'] = pd.Series(dtype=float)\n",
    "    \n",
    "    # Combine into single DataFrame\n",
    "    historical_df = pd.DataFrame(historical_data)\n",
    "    historical_df.index.name = 'Rider'\n",
    "    historical_df.reset_index(inplace=True)\n",
    "    \n",
    "    return historical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Each Year (2017-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_length_column(length_series):\n",
    "    \"\"\"Convert '118.5 km' to 118.5\"\"\"\n",
    "    def extract_numeric(x):\n",
    "        try:\n",
    "            if pd.isna(x):\n",
    "                return np.nan\n",
    "            # Extract first number from string\n",
    "            return float(str(x).split()[0])\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    return length_series.apply(extract_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(year):\n",
    "    \"\"\"Process a single year of race data\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING YEAR {year}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load race results\n",
    "    file_path = f'./data/results/Results{year}CatWT.csv'\n",
    "    results = pd.read_csv(file_path)\n",
    "    print(f\"1. Loaded {len(results)} race results for {year}\")\n",
    "    \n",
    "    # Calculate historical performance\n",
    "    print(f\"\\n2. Calculating historical performance features...\")\n",
    "    historical = calculate_historical_points(year)\n",
    "    print(f\"   Historical data shape: {historical.shape}\")\n",
    "    \n",
    "    # Merge with rider information\n",
    "    print(f\"\\n3. Merging with rider information...\")\n",
    "    data = results.merge(\n",
    "        rider_infos_clean,\n",
    "        left_on='Rider',\n",
    "        right_on='fullname',\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"   After rider merge: {data.shape}\")\n",
    "    \n",
    "    # Merge with historical data\n",
    "    data = data.merge(\n",
    "        historical,\n",
    "        left_on='Rider',\n",
    "        right_on='Rider',\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"   After historical merge: {data.shape}\")\n",
    "    \n",
    "    # Clean Rnk column - keep only numeric ranks\n",
    "    print(f\"\\n4. Cleaning Rnk column (removing DNF, OTL, etc.)...\")\n",
    "    initial_rows = len(data)\n",
    "    data['Rnk'] = pd.to_numeric(data['Rnk'], errors='coerce')\n",
    "    data = data.dropna(subset=['Rnk'])\n",
    "    data['Rnk'] = data['Rnk'].astype(int)\n",
    "    print(f\"   Removed {initial_rows - len(data)} non-numeric ranks\")\n",
    "    print(f\"   Remaining rows: {len(data)}\")\n",
    "    \n",
    "    # Clean Length column\n",
    "    print(f\"\\n5. Converting Length column to numeric...\")\n",
    "    data['Length'] = clean_length_column(data['Length'])\n",
    "    print(f\"   Sample lengths: {data['Length'].head().tolist()}\")\n",
    "    \n",
    "    # Drop Pnt column\n",
    "    if 'Pnt' in data.columns:\n",
    "        data = data.drop(columns=['Pnt'])\n",
    "        print(f\"\\n6. Dropped 'Pnt' column\")\n",
    "    \n",
    "    # Create binary target\n",
    "    print(f\"\\n7. Creating binary target (top 30 = 1, else 0)...\")\n",
    "    data['target'] = (data['Rnk'] <= 30).astype(int)\n",
    "    target_dist = data['target'].value_counts()\n",
    "    print(f\"   Target distribution:\")\n",
    "    print(f\"   - Top 30 (1): {target_dist.get(1, 0)} ({target_dist.get(1, 0)/len(data)*100:.1f}%)\")\n",
    "    print(f\"   - Outside top 30 (0): {target_dist.get(0, 0)} ({target_dist.get(0, 0)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # One-hot encode Stage_Type\n",
    "    print(f\"\\n8. One-hot encoding Stage_Type...\")\n",
    "    print(f\"   Unique Stage_Type values: {data['Stage_Type'].nunique()}\")\n",
    "    data = pd.get_dummies(data, columns=['Stage_Type'], drop_first=True)\n",
    "    stage_type_cols = [col for col in data.columns if col.startswith('Stage_Type_')]\n",
    "    print(f\"   Created {len(stage_type_cols)} dummy variables\")\n",
    "    \n",
    "    # Select final features\n",
    "    print(f\"\\n9. Selecting final features...\")\n",
    "    \n",
    "    features_from_results = ['GC', 'Age', 'Length']\n",
    "    features_physical = ['height', 'weight']\n",
    "    features_specialty = ['One day races', 'GC_specialty', 'Time trial', 'Sprint', 'Climber']\n",
    "    features_ranking = ['PCS Ranking', 'UCI World Ranking', 'Specials | All Time Ranking']\n",
    "    features_historical = ['sumres_1', 'sumres_2', 'sumres_3']\n",
    "    stage_type_dummies = [col for col in data.columns if col.startswith('Stage_Type_')]\n",
    "    \n",
    "    feature_cols = (features_from_results + features_physical + features_specialty + \n",
    "                   features_ranking + features_historical + stage_type_dummies)\n",
    "    \n",
    "    existing_features = [col for col in feature_cols if col in data.columns]\n",
    "    missing_features = [col for col in feature_cols if col not in data.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"   WARNING: Missing features: {missing_features}\")\n",
    "    \n",
    "    print(f\"   Selected {len(existing_features)} features\")\n",
    "    \n",
    "    final_cols = ['target'] + existing_features\n",
    "    data_final = data[final_cols].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"\\n10. Handling missing values...\")\n",
    "    missing_summary = data_final.isnull().sum()\n",
    "    missing_summary = missing_summary[missing_summary > 0]\n",
    "    if len(missing_summary) > 0:\n",
    "        print(f\"    Missing values before imputation:\")\n",
    "        print(missing_summary)\n",
    "        \n",
    "        numeric_cols = data_final.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col != 'target']\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if data_final[col].isnull().sum() > 0:\n",
    "                mean_val = data_final[col].mean()\n",
    "                data_final[col].fillna(mean_val, inplace=True)\n",
    "        \n",
    "        print(f\"    Imputed missing values with column means\")\n",
    "    else:\n",
    "        print(f\"    No missing values found\")\n",
    "    \n",
    "    # Drop any remaining rows with missing values\n",
    "    initial_rows = len(data_final)\n",
    "    data_final = data_final.dropna()\n",
    "    if len(data_final) < initial_rows:\n",
    "        print(f\"    Dropped {initial_rows - len(data_final)} rows with remaining missing values\")\n",
    "    \n",
    "    # Convert to float16\n",
    "    print(f\"\\n11. Converting to float16 for memory efficiency...\")\n",
    "    for col in data_final.columns:\n",
    "        if col != 'target':\n",
    "            data_final[col] = data_final[col].astype(np.float16)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    output_file = f'cleaned_data_{year}.csv'\n",
    "    data_final.to_csv(output_file, index=False)\n",
    "    print(f\"\\n12. ✓ Saved {output_file}\")\n",
    "    print(f\"    Final shape: {data_final.shape}\")\n",
    "    print(f\"    Features: {data_final.shape[1] - 1}\")\n",
    "    \n",
    "    return {\n",
    "        'year': year,\n",
    "        'total_rows': len(data_final),\n",
    "        'top_30_count': target_dist.get(1, 0),\n",
    "        'outside_top_30_count': target_dist.get(0, 0),\n",
    "        'features_count': data_final.shape[1] - 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING YEAR 2017\n",
      "============================================================\n",
      "1. Loaded 28565 race results for 2017\n",
      "\n",
      "2. Calculating historical performance features...\n",
      "  Loaded 2016: 851 riders with points\n",
      "  Loaded 2015: 814 riders with points\n",
      "  Loaded 2014: 799 riders with points\n",
      "   Historical data shape: (1103, 4)\n",
      "\n",
      "3. Merging with rider information...\n",
      "   After rider merge: (28565, 35)\n",
      "   After historical merge: (28565, 38)\n",
      "\n",
      "4. Cleaning Rnk column (removing DNF, OTL, etc.)...\n",
      "   Removed 1867 non-numeric ranks\n",
      "   Remaining rows: 26698\n",
      "\n",
      "5. Converting Length column to numeric...\n",
      "   Sample lengths: [118.5, 118.5, 118.5, 118.5, 118.5]\n",
      "\n",
      "6. Dropped 'Pnt' column\n",
      "\n",
      "7. Creating binary target (top 30 = 1, else 0)...\n",
      "   Target distribution:\n",
      "   - Top 30 (1): 5182 (19.4%)\n",
      "   - Outside top 30 (0): 21516 (80.6%)\n",
      "\n",
      "8. One-hot encoding Stage_Type...\n",
      "   Unique Stage_Type values: 2\n",
      "   Created 1 dummy variables\n",
      "\n",
      "9. Selecting final features...\n",
      "   Selected 17 features\n",
      "\n",
      "10. Handling missing values...\n",
      "    Missing values before imputation:\n",
      "GC                              2438\n",
      "height                          9139\n",
      "weight                          9139\n",
      "One day races                  10674\n",
      "GC_specialty                   10674\n",
      "Time trial                     10674\n",
      "Sprint                         10674\n",
      "Climber                        10674\n",
      "PCS Ranking                    11847\n",
      "UCI World Ranking              11416\n",
      "Specials | All Time Ranking    21022\n",
      "sumres_1                        2988\n",
      "sumres_2                        5370\n",
      "sumres_3                        7621\n",
      "dtype: int64\n",
      "    Imputed missing values with column means\n",
      "\n",
      "11. Converting to float16 for memory efficiency...\n",
      "\n",
      "12. ✓ Saved cleaned_data_2017.csv\n",
      "    Final shape: (26698, 18)\n",
      "    Features: 17\n",
      "\n",
      "============================================================\n",
      "PROCESSING YEAR 2018\n",
      "============================================================\n",
      "1. Loaded 26184 race results for 2018\n",
      "\n",
      "2. Calculating historical performance features...\n",
      "  Loaded 2017: 924 riders with points\n",
      "  Loaded 2016: 851 riders with points\n",
      "  Loaded 2015: 814 riders with points\n",
      "   Historical data shape: (1210, 4)\n",
      "\n",
      "3. Merging with rider information...\n",
      "   After rider merge: (26184, 35)\n",
      "   After historical merge: (26184, 38)\n",
      "\n",
      "4. Cleaning Rnk column (removing DNF, OTL, etc.)...\n",
      "   Removed 1767 non-numeric ranks\n",
      "   Remaining rows: 24417\n",
      "\n",
      "5. Converting Length column to numeric...\n",
      "   Sample lengths: [145.0, 145.0, 145.0, 145.0, 145.0]\n",
      "\n",
      "6. Dropped 'Pnt' column\n",
      "\n",
      "7. Creating binary target (top 30 = 1, else 0)...\n",
      "   Target distribution:\n",
      "   - Top 30 (1): 5115 (20.9%)\n",
      "   - Outside top 30 (0): 19302 (79.1%)\n",
      "\n",
      "8. One-hot encoding Stage_Type...\n",
      "   Unique Stage_Type values: 2\n",
      "   Created 1 dummy variables\n",
      "\n",
      "9. Selecting final features...\n",
      "   Selected 17 features\n",
      "\n",
      "10. Handling missing values...\n",
      "    Missing values before imputation:\n",
      "GC                              2084\n",
      "height                          5481\n",
      "weight                          5481\n",
      "One day races                   6823\n",
      "GC_specialty                    6823\n",
      "Time trial                      6823\n",
      "Sprint                          6823\n",
      "Climber                         6823\n",
      "PCS Ranking                     7842\n",
      "UCI World Ranking               7530\n",
      "Specials | All Time Ranking    18141\n",
      "sumres_1                        1841\n",
      "sumres_2                        4648\n",
      "sumres_3                        6563\n",
      "dtype: int64\n",
      "    Imputed missing values with column means\n",
      "\n",
      "11. Converting to float16 for memory efficiency...\n",
      "\n",
      "12. ✓ Saved cleaned_data_2018.csv\n",
      "    Final shape: (24417, 18)\n",
      "    Features: 17\n",
      "\n",
      "============================================================\n",
      "PROCESSING YEAR 2019\n",
      "============================================================\n",
      "1. Loaded 26477 race results for 2019\n",
      "\n",
      "2. Calculating historical performance features...\n",
      "  Loaded 2018: 918 riders with points\n",
      "  Loaded 2017: 924 riders with points\n",
      "  Loaded 2016: 851 riders with points\n",
      "   Historical data shape: (1244, 4)\n",
      "\n",
      "3. Merging with rider information...\n",
      "   After rider merge: (26477, 35)\n",
      "   After historical merge: (26477, 38)\n",
      "\n",
      "4. Cleaning Rnk column (removing DNF, OTL, etc.)...\n",
      "   Removed 1628 non-numeric ranks\n",
      "   Remaining rows: 24849\n",
      "\n",
      "5. Converting Length column to numeric...\n",
      "   Sample lengths: [129.1, 129.1, 129.1, 129.1, 129.1]\n",
      "\n",
      "6. Dropped 'Pnt' column\n",
      "\n",
      "7. Creating binary target (top 30 = 1, else 0)...\n",
      "   Target distribution:\n",
      "   - Top 30 (1): 5219 (21.0%)\n",
      "   - Outside top 30 (0): 19630 (79.0%)\n",
      "\n",
      "8. One-hot encoding Stage_Type...\n",
      "   Unique Stage_Type values: 2\n",
      "   Created 1 dummy variables\n",
      "\n",
      "9. Selecting final features...\n",
      "   Selected 17 features\n",
      "\n",
      "10. Handling missing values...\n",
      "    Missing values before imputation:\n",
      "GC                              2328\n",
      "height                          3425\n",
      "weight                          3425\n",
      "One day races                   4939\n",
      "GC_specialty                    4939\n",
      "Time trial                      4939\n",
      "Sprint                          4939\n",
      "Climber                         4939\n",
      "PCS Ranking                     6021\n",
      "UCI World Ranking               5850\n",
      "Specials | All Time Ranking    17333\n",
      "sumres_1                        1907\n",
      "sumres_2                        4147\n",
      "sumres_3                        7106\n",
      "dtype: int64\n",
      "    Imputed missing values with column means\n",
      "\n",
      "11. Converting to float16 for memory efficiency...\n",
      "\n",
      "12. ✓ Saved cleaned_data_2019.csv\n",
      "    Final shape: (24849, 18)\n",
      "    Features: 17\n",
      "\n",
      "============================================================\n",
      "PROCESSING YEAR 2020\n",
      "============================================================\n",
      "1. Loaded 17153 race results for 2020\n",
      "\n",
      "2. Calculating historical performance features...\n",
      "  Loaded 2019: 944 riders with points\n",
      "  Loaded 2018: 918 riders with points\n",
      "  Loaded 2017: 924 riders with points\n",
      "   Historical data shape: (1248, 4)\n",
      "\n",
      "3. Merging with rider information...\n",
      "   After rider merge: (17153, 35)\n",
      "   After historical merge: (17153, 38)\n",
      "\n",
      "4. Cleaning Rnk column (removing DNF, OTL, etc.)...\n",
      "   Removed 1097 non-numeric ranks\n",
      "   Remaining rows: 16056\n",
      "\n",
      "5. Converting Length column to numeric...\n",
      "   Sample lengths: [150.0, 150.0, 150.0, 150.0, 150.0]\n",
      "\n",
      "6. Dropped 'Pnt' column\n",
      "\n",
      "7. Creating binary target (top 30 = 1, else 0)...\n",
      "   Target distribution:\n",
      "   - Top 30 (1): 3330 (20.7%)\n",
      "   - Outside top 30 (0): 12726 (79.3%)\n",
      "\n",
      "8. One-hot encoding Stage_Type...\n",
      "   Unique Stage_Type values: 2\n",
      "   Created 1 dummy variables\n",
      "\n",
      "9. Selecting final features...\n",
      "   Selected 17 features\n",
      "\n",
      "10. Handling missing values...\n",
      "    Missing values before imputation:\n",
      "GC                              1069\n",
      "height                          1789\n",
      "weight                          1789\n",
      "One day races                   2834\n",
      "GC_specialty                    2834\n",
      "Time trial                      2834\n",
      "Sprint                          2834\n",
      "Climber                         2834\n",
      "PCS Ranking                     3652\n",
      "UCI World Ranking               3561\n",
      "Specials | All Time Ranking    11359\n",
      "sumres_1                        1503\n",
      "sumres_2                        2783\n",
      "sumres_3                        4057\n",
      "dtype: int64\n",
      "    Imputed missing values with column means\n",
      "\n",
      "11. Converting to float16 for memory efficiency...\n",
      "\n",
      "12. ✓ Saved cleaned_data_2020.csv\n",
      "    Final shape: (16056, 18)\n",
      "    Features: 17\n",
      "\n",
      "============================================================\n",
      "PROCESSING YEAR 2021\n",
      "============================================================\n",
      "1. Loaded 30031 race results for 2021\n",
      "\n",
      "2. Calculating historical performance features...\n",
      "  Loaded 2020: 812 riders with points\n",
      "  Loaded 2019: 944 riders with points\n",
      "  Loaded 2018: 918 riders with points\n",
      "   Historical data shape: (1229, 4)\n",
      "\n",
      "3. Merging with rider information...\n",
      "   After rider merge: (30031, 35)\n",
      "   After historical merge: (30031, 38)\n",
      "\n",
      "4. Cleaning Rnk column (removing DNF, OTL, etc.)...\n",
      "   Removed 1790 non-numeric ranks\n",
      "   Remaining rows: 28241\n",
      "\n",
      "5. Converting Length column to numeric...\n",
      "   Sample lengths: [176.0, 176.0, 176.0, 176.0, 176.0]\n",
      "\n",
      "6. Dropped 'Pnt' column\n",
      "\n",
      "7. Creating binary target (top 30 = 1, else 0)...\n",
      "   Target distribution:\n",
      "   - Top 30 (1): 5819 (20.6%)\n",
      "   - Outside top 30 (0): 22422 (79.4%)\n",
      "\n",
      "8. One-hot encoding Stage_Type...\n",
      "   Unique Stage_Type values: 2\n",
      "   Created 1 dummy variables\n",
      "\n",
      "9. Selecting final features...\n",
      "   Selected 17 features\n",
      "\n",
      "10. Handling missing values...\n",
      "    Missing values before imputation:\n",
      "GC                              1915\n",
      "height                          4100\n",
      "weight                          4100\n",
      "One day races                   6372\n",
      "GC_specialty                    6372\n",
      "Time trial                      6372\n",
      "Sprint                          6372\n",
      "Climber                         6372\n",
      "PCS Ranking                     7906\n",
      "UCI World Ranking               7562\n",
      "Specials | All Time Ranking    20481\n",
      "sumres_1                        2889\n",
      "sumres_2                        4832\n",
      "sumres_3                        7188\n",
      "dtype: int64\n",
      "    Imputed missing values with column means\n",
      "\n",
      "11. Converting to float16 for memory efficiency...\n",
      "\n",
      "12. ✓ Saved cleaned_data_2021.csv\n",
      "    Final shape: (28241, 18)\n",
      "    Features: 17\n"
     ]
    }
   ],
   "source": [
    "# Process all years (2017-2021 only - 2013 data missing, so 2016 excluded)\n",
    "years = [2017, 2018, 2019, 2020, 2021]\n",
    "summary_data = []\n",
    "\n",
    "for year in years:\n",
    "    try:\n",
    "        summary = process_year(year)\n",
    "        summary_data.append(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR processing year {year}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPROCESSING COMPLETE - SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "Note: Years 2017-2021 processed (5 datasets)\n",
      "2016 excluded because Results2013CatWT.csv is missing,\n",
      "which means sumres_3 (3-year historical points) would be incomplete.\n",
      "All 5 datasets have complete 3-year historical features.\n",
      "\n",
      "\n",
      "Dataset Summary:\n",
      " year  total_rows  top_30_count  outside_top_30_count  features_count\n",
      " 2017       26698          5182                 21516              17\n",
      " 2018       24417          5115                 19302              17\n",
      " 2019       24849          5219                 19630              17\n",
      " 2020       16056          3330                 12726              17\n",
      " 2021       28241          5819                 22422              17\n",
      "\n",
      "================================================================================\n",
      "OUTPUT FILES CREATED:\n",
      "================================================================================\n",
      "  ✓ cleaned_data_2017.csv\n",
      "  ✓ cleaned_data_2018.csv\n",
      "  ✓ cleaned_data_2019.csv\n",
      "  ✓ cleaned_data_2020.csv\n",
      "  ✓ cleaned_data_2021.csv\n",
      "\n",
      "================================================================================\n",
      "NEXT STEPS:\n",
      "================================================================================\n",
      "1. Validate the cleaned CSV files\n",
      "2. Share with team members (David, Youri, Iris)\n",
      "3. Begin modeling with Random Forest and Gradient Boosting\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETE - SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Years 2017-2021 processed (5 datasets)\")\n",
    "print(\"2016 excluded because Results2013CatWT.csv is missing,\")\n",
    "print(\"which means sumres_3 (3-year historical points) would be incomplete.\")\n",
    "print(\"All 5 datasets have complete 3-year historical features.\\n\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OUTPUT FILES CREATED:\")\n",
    "print(\"=\"*80)\n",
    "for year in years:\n",
    "    print(f\"  ✓ cleaned_data_{year}.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Validate the cleaned CSV files\")\n",
    "print(\"2. Share with team members (David, Youri, Iris)\")\n",
    "print(\"3. Begin modeling with Random Forest and Gradient Boosting\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION CHECK - Loading one file to verify structure...\n",
      "================================================================================\n",
      "\n",
      "File: cleaned_data_2019.csv\n",
      "Shape: (24849, 18)\n",
      "\n",
      "Columns (18):\n",
      "['target', 'GC', 'Age', 'Length', 'height', 'weight', 'One day races', 'GC_specialty', 'Time trial', 'Sprint', 'Climber', 'PCS Ranking', 'UCI World Ranking', 'Specials | All Time Ranking', 'sumres_1', 'sumres_2', 'sumres_3', 'Stage_Type_RR']\n",
      "\n",
      "First 3 rows:\n",
      "   target   GC   Age  Length  height  weight  One day races  GC_specialty  \\\n",
      "0       1  1.0  29.0   129.1    1.78    67.0          169.0         327.0   \n",
      "1       1  2.0  25.0   129.1    1.99    90.0           18.0          36.0   \n",
      "2       1  5.0  24.0   129.1    1.69    68.0         4550.0         899.0   \n",
      "\n",
      "   Time trial  Sprint  Climber  PCS Ranking  UCI World Ranking  \\\n",
      "0       430.0   581.0    150.0        235.0              394.0   \n",
      "1       155.0    92.0     24.0       1459.0             3028.0   \n",
      "2       160.0  8680.0    512.0         59.0               68.0   \n",
      "\n",
      "   Specials | All Time Ranking  sumres_1  sumres_2  sumres_3  Stage_Type_RR  \n",
      "0                       1007.5    1580.0     657.0     112.0            1.0  \n",
      "1                       1007.5      91.0      77.0       0.0            1.0  \n",
      "2                         84.0     102.0     108.0       5.0            1.0  \n",
      "\n",
      "Data types:\n",
      "float64    17\n",
      "int64       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    19630\n",
      "1     5219\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ File structure validated successfully!\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING PIPELINE COMPLETE! DATA=CLEAN\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION CHECK - Loading one file to verify structure...\")\n",
    "print(\"=\"*80)\n",
    "sample_file = 'cleaned_data_2019.csv'\n",
    "try:\n",
    "    df_check = pd.read_csv(sample_file)\n",
    "    print(f\"\\nFile: {sample_file}\")\n",
    "    print(f\"Shape: {df_check.shape}\")\n",
    "    print(f\"\\nColumns ({len(df_check.columns)}):\")\n",
    "    print(list(df_check.columns))\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(df_check.head(3))\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df_check.dtypes.value_counts())\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(df_check['target'].value_counts())\n",
    "    print(\"\\n✓ File structure validated successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during validation: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING PIPELINE COMPLETE! DATA=CLEAN\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
